We revisit and replicate the VulRepair study, a T5-based neural machine translation model designed to automatically repair software vulnerabilities. We re-implement all ten model variants and conduct a comprehensive evaluation to investigate four key research questions (RQs): model accuracy (RQ1), impact of pre-training (RQ2), benefits of BPE tokenization (RQ3), and the contribution of each VulRepair component (RQ4). Through model replication and dataset deduplication, we assess how each design choice contributes to model performance. Our findings confirm the original study's conclusions and further reveal how duplications in the dataset can influence reported accuracy. This study contributes reproducibility insights and extends the VulRepair evaluation to all 10 variants using deduplicated data.